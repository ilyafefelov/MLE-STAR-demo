#!/usr/bin/env python
"""
–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ task —Ñ–∞–π–ª—ñ–≤ –¥–ª—è MLE-STAR –∑ sklearn –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤.
–°—Ç–≤–æ—Ä—é—î —Å—Ç—Ä—É–∫—Ç—É—Ä—É:
  tasks/
    breast_cancer/
      task_description.txt
      train.csv
      test.csv
    wine/...
    digits/...
    iris/...

–ê–≤—Ç–æ—Ä: –§–µ—Ñ–µ–ª–æ–≤ –Ü–ª–ª—è –û–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–∏—á
"""

import sys
from pathlib import Path
import pandas as pd
from sklearn.model_selection import train_test_split

# –î–æ–¥–∞—î–º–æ src –¥–æ —à–ª—è—Ö—É
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.mle_star_ablation.datasets import DatasetLoader


def create_task_files(dataset_name: str, output_base: Path):
    """
    –°—Ç–≤–æ—Ä—é—î task —Ñ–∞–π–ª–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É.
    
    Args:
        dataset_name: –ù–∞–∑–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç—É (breast_cancer, wine, digits, iris)
        output_base: –ë–∞–∑–æ–≤–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    """
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É
    print(f"\nüì¶ Processing {dataset_name}...")
    X_train, X_test, y_train, y_test = DatasetLoader.load_dataset(dataset_name)
    info = DatasetLoader.get_dataset_info(dataset_name)
    
    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è feature names –∑—ñ sklearn –¥–∞—Ç–∞—Å–µ—Ç—É
    loader = DatasetLoader.BUILTIN_DATASETS[dataset_name]
    dataset = loader()
    feature_names = list(dataset.feature_names)
    
    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
    task_dir = output_base / dataset_name
    task_dir.mkdir(parents=True, exist_ok=True)
    
    # –§–æ—Ä–º—É–≤–∞–Ω–Ω—è DataFrame (–≤–∂–µ –º–∞—î–º–æ train/test split)
    train_df = pd.DataFrame(X_train, columns=feature_names)
    train_df['target'] = y_train
    
    test_df = pd.DataFrame(X_test, columns=feature_names)
    # test.csv –ù–ï –º—ñ—Å—Ç–∏—Ç—å target (—è–∫ —É Kaggle)
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è CSV
    train_csv = task_dir / "train.csv"
    test_csv = task_dir / "test.csv"
    
    train_df.to_csv(train_csv, index=False)
    test_df.to_csv(test_csv, index=False)
    
    print(f"  ‚úÖ Created {train_csv} ({len(train_df)} samples)")
    print(f"  ‚úÖ Created {test_csv} ({len(test_df)} samples)")
    
    # –û–ø–∏—Å –¥–∞—Ç–∞—Å–µ—Ç—É
    dataset_descriptions = {
        'breast_cancer': 'Breast Cancer Wisconsin (Diagnostic) - predict if tumor is benign or malignant',
        'wine': 'Wine Recognition - classify wines from different cultivars',
        'digits': 'Optical Recognition of Handwritten Digits - classify handwritten digits 0-9',
        'iris': 'Iris Flower Classification - classify iris species based on sepal/petal measurements'
    }
    
    # –û–ø–∏—Å –∑–∞–¥–∞—á—ñ
    task_description = f"""# Task

Predict the target class for the {dataset_name} dataset.

This is a classification problem with {info['n_classes']} classes.

Dataset: {dataset_descriptions.get(dataset_name, dataset_name)}

# Metric

accuracy

Note: The model should predict class labels (integers from 0 to {info['n_classes']-1}).

# Submission Format
```
target
1
0
2
etc.
```

# Dataset

train.csv
```
{','.join(feature_names)},target
{','.join(map(str, X_train[0]))},{y_train[0]}
{','.join(map(str, X_train[1]))},{y_train[1]}
{','.join(map(str, X_train[2]))},{y_train[2]}
etc.
```

test.csv
```
{','.join(feature_names)}
{','.join(map(str, X_test[0]))}
{','.join(map(str, X_test[1]))}
{','.join(map(str, X_test[2]))}
etc.
```

# Additional Information

- Number of samples: {info['n_samples']} ({len(train_df)} train, {len(test_df)} test)
- Number of features: {info['n_features']}
- Number of classes: {info['n_classes']}
- Feature types: All features are numeric (real-valued)

# Objective

Build a scikit-learn pipeline that:
1. Preprocesses the data (handle missing values, scaling, etc.)
2. Optionally performs feature engineering
3. Trains a classification model
4. Achieves high accuracy on the test set

The pipeline should be robust and follow machine learning best practices.
"""
    
    task_desc_file = task_dir / "task_description.txt"
    with open(task_desc_file, 'w', encoding='utf-8') as f:
        f.write(task_description)
    
    print(f"  ‚úÖ Created {task_desc_file}")
    
    # –¢–∞–∫–æ–∂ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ test labels –æ–∫—Ä–µ–º–æ (–¥–ª—è evaluation)
    test_labels_file = task_dir / "test_labels.csv"
    pd.DataFrame({'target': y_test}).to_csv(test_labels_file, index=False)
    print(f"  ‚úÖ Created {test_labels_file} (for evaluation only)")


def main():
    # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è —à–ª—è—Ö—ñ–≤
    mle_star_root = Path(__file__).parent.parent / "adk-samples" / "python" / "agents" / "machine-learning-engineering"
    tasks_dir = mle_star_root / "machine_learning_engineering" / "tasks"
    
    if not tasks_dir.exists():
        print(f"‚ùå MLE-STAR tasks directory not found: {tasks_dir}")
        print("   Make sure adk-samples is cloned in the project root")
        return
    
    print("="*80)
    print("PREPARING MLE-STAR TASKS FROM SKLEARN DATASETS")
    print("="*80)
    print(f"Output directory: {tasks_dir}")
    
    # –î–∞—Ç–∞—Å–µ—Ç–∏
    datasets = ['breast_cancer', 'wine', 'digits', 'iris']
    
    for dataset_name in datasets:
        try:
            create_task_files(dataset_name, tasks_dir)
        except Exception as e:
            print(f"  ‚ùå Error processing {dataset_name}: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "="*80)
    print("‚úÖ TASK PREPARATION COMPLETE")
    print("="*80)
    print("\nNext steps:")
    print("1. Review task files in:")
    print(f"   {tasks_dir}")
    print("2. Run MLE-STAR for each dataset:")
    print("   cd adk-samples/python/agents/machine-learning-engineering")
    print("   poetry run adk run machine_learning_engineering --task breast_cancer")
    print("="*80)


if __name__ == "__main__":
    main()
