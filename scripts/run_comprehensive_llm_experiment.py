#!/usr/bin/env python3
"""
ÐšÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ð¸Ð¹ ÐµÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚: Simple vs MLE-STAR Prompt vs ADK MLE-STAR Agent
=========================================================================

ÐŸÐ¾Ñ€Ñ–Ð²Ð½ÑÐ½Ð½Ñ Ñ‚Ñ€ÑŒÐ¾Ñ… Ð¿Ñ–Ð´Ñ…Ð¾Ð´Ñ–Ð² Ð´Ð¾ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ñ–Ñ— ML pipelines:
1. Simple Prompt (Gemini / GPT-4o) - Ð¼Ñ–Ð½Ñ–Ð¼Ð°Ð»ÑŒÐ½Ñ– Ñ–Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ñ–Ñ—
2. MLE-STAR Prompt (Gemini / GPT-4o) - Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ– Ñ–Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ñ–Ñ— Ð· MLE-STAR
3. ADK MLE-STAR Agent - Ð¿Ð¾Ð²Ð½Ð¾Ñ†Ñ–Ð½Ð½Ð¸Ð¹ multi-agent system

"""

import os
import sys
import json
import time
import re
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any, Tuple, Callable

import numpy as np
import pandas as pd
from sklearn.datasets import load_iris, load_breast_cancer, fetch_california_housing
from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ API ÐºÐ»ÑŽÑ‡Ñ–Ð²
from dotenv import load_dotenv
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Ð”Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ñ–Ñ—
PROJECT_ROOT = Path(__file__).parent.parent
RESULTS_DIR = PROJECT_ROOT / "results" / "llm_comparison"
GENERATED_PIPELINES_DIR = PROJECT_ROOT / "generated_pipelines"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)


# ============================================================================
# ÐŸÐ ÐžÐœÐŸÐ¢Ð˜
# ============================================================================

SIMPLE_PROMPT_TEMPLATE = """
Ð¡Ñ‚Ð²Ð¾Ñ€Ð¸ sklearn Pipeline Ð´Ð»Ñ {task_type} Ð½Ð° Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñ– {dataset_name}.

Ð”Ð°Ð½Ñ–: {n_samples} Ð·Ñ€Ð°Ð·ÐºÑ–Ð², {n_features} Ð¾Ð·Ð½Ð°Ðº{class_info}.
Ð’ÐÐ–Ð›Ð˜Ð’Ðž: Ð”Ð°Ð½Ñ– Ð¿ÐµÑ€ÐµÐ´Ð°ÑŽÑ‚ÑŒÑÑ ÑÐº numpy array (X, y), ÐÐ• pandas DataFrame!

ÐŸÐ¾Ð²ÐµÑ€Ð½Ð¸ Ð¢Ð†Ð›Ð¬ÐšÐ˜ Python ÐºÐ¾Ð´:
- Ð£ÑÑ– Ñ–Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸ ÐÐ ÐŸÐžÐ§ÐÐ¢ÐšÐ£ Ñ„Ð°Ð¹Ð»Ñƒ (Ð¿ÐµÑ€ÐµÐ´ def)
- ÐÐ• Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹ type hints Ñƒ ÑÐ¸Ð³Ð½Ð°Ñ‚ÑƒÑ€Ñ– Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ—
- ÐŸÑ€Ð¾ÑÑ‚Ð¸Ð¹ pipeline: StandardScaler + Ð¼Ð¾Ð´ÐµÐ»ÑŒ (Ð¼Ð°ÐºÑÐ¸Ð¼ÑƒÐ¼ 2-3 ÐºÑ€Ð¾ÐºÐ¸)

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC  # Ð°Ð±Ð¾ Ñ–Ð½ÑˆÐ° Ð¼Ð¾Ð´ÐµÐ»ÑŒ

def build_full_pipeline(random_state=42):
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('model', SVC(random_state=random_state))
    ])
    return pipeline
```
"""

MLE_STAR_PROMPT_TEMPLATE = """
Ð¢Ð¸ ÐµÐºÑÐ¿ÐµÑ€Ñ‚ Machine Learning Engineer. Ð“ÐµÐ½ÐµÑ€ÑƒÐ¹ Ð¿Ð¾Ð²Ð½Ð¸Ð¹ scikit-learn ML pipeline.

Ð”ÐÐ¢ÐÐ¡Ð•Ð¢: {dataset_name}
- Ð—Ñ€Ð°Ð·ÐºÑ–Ð²: {n_samples}
- ÐžÐ·Ð½Ð°Ðº: {n_features}
{class_info}

Ð’Ð˜ÐœÐžÐ“Ð˜:
1. Pipeline Ð¼Ð°Ñ” Ð¼Ñ–ÑÑ‚Ð¸Ñ‚Ð¸:
   - preprocessor: SimpleImputer + StandardScaler
   - feature_engineering: PCA Ð°Ð±Ð¾ PolynomialFeatures
   - model: ÐžÐ±ÐµÑ€Ð¸ ÐÐÐ™ÐšÐ ÐÐ©Ð£ Ð¼Ð¾Ð´ÐµÐ»ÑŒ (RandomForest, GradientBoosting, SVM, etc.)

2. Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹ Ð°Ð½ÑÐ°Ð¼Ð±Ð»ÑŽÐ²Ð°Ð½Ð½Ñ (VotingClassifier/VotingRegressor) ÑÐºÑ‰Ð¾ Ñ†Ðµ Ð¿Ð¾ÐºÑ€Ð°Ñ‰Ð¸Ñ‚ÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚

3. ÐÐ°Ð»Ð°ÑˆÑ‚ÑƒÐ¹ Ð³Ñ–Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸ Ð´Ð»Ñ Ñ†ÑŒÐ¾Ð³Ð¾ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾Ð³Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ

4. Ð”Ð¾Ð´Ð°Ð¹ ÐºÐ¾Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ñ– Ð· Ð¾Ð±Ò‘Ñ€ÑƒÐ½Ñ‚ÑƒÐ²Ð°Ð½Ð½ÑÐ¼ Ð²Ð¸Ð±Ð¾Ñ€Ñƒ

5. Ð’ÐÐ–Ð›Ð˜Ð’Ðž: Ð”Ð°Ð½Ñ– Ð¿ÐµÑ€ÐµÐ´Ð°ÑŽÑ‚ÑŒÑÑ ÑÐº numpy array (X, y), ÐÐ• DataFrame. 
   ÐÐ• Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹ ColumnTransformer Ð· Ñ–Ð¼ÐµÐ½Ð°Ð¼Ð¸ ÐºÐ¾Ð»Ð¾Ð½Ð¾Ðº!
   ÐŸÑ€Ð°Ñ†ÑŽÐ¹ Ð· ÑƒÑÑ–Ð¼Ð° Ð¾Ð·Ð½Ð°ÐºÐ°Ð¼Ð¸ Ð¾Ð´Ñ€Ð°Ð·Ñƒ (StandardScaler Ð´Ð»Ñ Ð²ÑÑ–Ñ…).

6. Ð—ÐÐ‘ÐžÐ ÐžÐÐ•ÐÐž: GridSearchCV, RandomizedSearchCV (Ð·Ð°Ð½Ð°Ð´Ñ‚Ð¾ Ð¿Ð¾Ð²Ñ–Ð»ÑŒÐ½Ð¾)
   Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹ Ñ„Ñ–ÐºÑÐ¾Ð²Ð°Ð½Ñ– Ð³Ñ–Ð¿ÐµÑ€Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¸.

Ð¤ÐžÐ ÐœÐÐ¢ Ð’Ð†Ð”ÐŸÐžÐ’Ð†Ð”Ð† - Ð¢Ð†Ð›Ð¬ÐšÐ˜ ÐšÐžÐ”:
- Ð£ÑÑ– Ñ–Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸ ÐÐ ÐŸÐžÐ§ÐÐ¢ÐšÐ£ Ñ„Ð°Ð¹Ð»Ñƒ
- ÐÐ• Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÐ¹ type hints (-> Pipeline)
- Ð¤ÑƒÐ½ÐºÑ†Ñ–Ñ Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ” sklearn.pipeline.Pipeline

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
# ... Ñ–Ð½ÑˆÑ– Ñ–Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸

def build_full_pipeline(random_state=42):
    # Ñ‚Ð²Ñ–Ð¹ ÐºÐ¾Ð´
    return pipeline
```
"""


# ============================================================================
# Ð”ÐÐ¢ÐÐ¡Ð•Ð¢Ð˜
# ============================================================================

def get_dataset_info(dataset_name: str) -> Dict[str, Any]:
    """Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÑƒÑ” Ð´Ð°Ñ‚Ð°ÑÐµÑ‚ Ñ– Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ” Ð¹Ð¾Ð³Ð¾ Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–ÑŽ."""
    if dataset_name == "iris":
        data = load_iris()
        task_type = "ÐºÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ—"
        class_info = f", {len(np.unique(data.target))} ÐºÐ»Ð°ÑÑ–Ð²"
        is_classification = True
    elif dataset_name == "breast_cancer":
        data = load_breast_cancer()
        task_type = "ÐºÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ—"
        class_info = f", {len(np.unique(data.target))} ÐºÐ»Ð°ÑÑ–Ð²"
        is_classification = True
    elif dataset_name == "california_housing":
        data = fetch_california_housing()
        task_type = "Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ—"
        class_info = ""
        is_classification = False
    else:
        raise ValueError(f"ÐÐµÐ²Ñ–Ð´Ð¾Ð¼Ð¸Ð¹ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚: {dataset_name}")
    
    return {
        "X": data.data,
        "y": data.target,
        "n_samples": data.data.shape[0],
        "n_features": data.data.shape[1],
        "task_type": task_type,
        "class_info": class_info,
        "is_classification": is_classification,
        "dataset_name": dataset_name,
    }


def format_prompt(template: str, info: Dict[str, Any]) -> str:
    """Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ÑƒÑ” Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð· Ñ–Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ñ–Ñ”ÑŽ Ð¿Ñ€Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚."""
    return template.format(
        dataset_name=info["dataset_name"],
        n_samples=info["n_samples"],
        n_features=info["n_features"],
        task_type=info["task_type"],
        class_info=info["class_info"],
    )


# ============================================================================
# LLM API Ð’Ð˜ÐšÐ›Ð˜ÐšÐ˜
# ============================================================================

def call_gemini(prompt: str, model: str = "gemini-2.0-flash") -> str:
    """Ð’Ð¸ÐºÐ»Ð¸Ðº Gemini API."""
    import google.generativeai as genai
    
    genai.configure(api_key=GEMINI_API_KEY)
    model_obj = genai.GenerativeModel(model)
    
    response = model_obj.generate_content(prompt)
    return response.text


def call_openai(prompt: str, model: str = "gpt-4o") -> str:
    """Ð’Ð¸ÐºÐ»Ð¸Ðº OpenAI API."""
    from openai import OpenAI
    
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "Ð¢Ð¸ ÐµÐºÑÐ¿ÐµÑ€Ñ‚ Machine Learning Engineer. Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ð°Ð¹ Ð¢Ð†Ð›Ð¬ÐšÐ˜ Python ÐºÐ¾Ð´Ð¾Ð¼."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.7,
    )
    return response.choices[0].message.content


def extract_code(response: str) -> str:
    """Ð’Ð¸Ñ‚ÑÐ³ÑƒÑ” Python ÐºÐ¾Ð´ Ð· Ð²Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´Ñ– LLM."""
    # Ð¨ÑƒÐºÐ°Ñ”Ð¼Ð¾ ÐºÐ¾Ð´ Ð¼Ñ–Ð¶ ```python Ñ– ```
    pattern = r"```python\s*(.*?)\s*```"
    matches = re.findall(pattern, response, re.DOTALL)
    
    if matches:
        return matches[0].strip()
    
    # Ð¯ÐºÑ‰Ð¾ Ð½ÐµÐ¼Ð°Ñ” Ð¼Ð°Ñ€ÐºÐµÑ€Ñ–Ð², ÑˆÑƒÐºÐ°Ñ”Ð¼Ð¾ def build_full_pipeline
    if "def build_full_pipeline" in response:
        start = response.find("def build_full_pipeline")
        # Ð—Ð½Ð°Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ ÐºÑ–Ð½ÐµÑ†ÑŒ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ— (Ð½Ð°ÑÑ‚ÑƒÐ¿Ð½Ð¸Ð¹ def Ð°Ð±Ð¾ ÐºÑ–Ð½ÐµÑ†ÑŒ)
        end = response.find("\ndef ", start + 1)
        if end == -1:
            end = len(response)
        return response[start:end].strip()
    
    return response


# ============================================================================
# Ð’Ð˜ÐšÐžÐÐÐÐÐ¯ ÐšÐžÐ”Ð£
# ============================================================================

def execute_code_and_get_pipeline(code: str, random_state: int = 42, timeout_sec: int = 30) -> Optional[Pipeline]:
    """Ð’Ð¸ÐºÐ¾Ð½ÑƒÑ” ÐºÐ¾Ð´ Ñ– Ð¿Ð¾Ð²ÐµÑ€Ñ‚Ð°Ñ” Pipeline Ð· timeout."""
    import threading
    
    namespace = {}
    result = {"pipeline": None, "error": None}
    
    def run_code():
        try:
            exec(code, namespace)
            
            if "build_full_pipeline" not in namespace:
                result["error"] = "Ð¤ÑƒÐ½ÐºÑ†Ñ–Ñ build_full_pipeline Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð°"
                return
            
            result["pipeline"] = namespace["build_full_pipeline"](random_state=random_state)
        except Exception as e:
            result["error"] = str(e)
    
    thread = threading.Thread(target=run_code)
    thread.start()
    thread.join(timeout=timeout_sec)
    
    if thread.is_alive():
        print(f"  âš ï¸ Timeout ({timeout_sec}Ñ) - ÐºÐ¾Ð´ Ð²Ð¸ÐºÐ¾Ð½ÑƒÑ”Ñ‚ÑŒÑÑ Ð·Ð°Ð½Ð°Ð´Ñ‚Ð¾ Ð´Ð¾Ð²Ð³Ð¾")
        return None
    
    if result["error"]:
        print(f"  âš ï¸ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° Ð²Ð¸ÐºÐ¾Ð½Ð°Ð½Ð½Ñ ÐºÐ¾Ð´Ñƒ: {result['error']}")
        return None
    
    return result["pipeline"]


def extract_model_from_adk_script(code: str) -> Optional[Any]:
    """
    Ð’Ð¸Ñ‚ÑÐ³ÑƒÑ” Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð· ADK MLE-STAR ÑÐºÑ€Ð¸Ð¿Ñ‚Ñƒ.
    ADK Ð³ÐµÐ½ÐµÑ€ÑƒÑ” Ð¿Ð¾Ð²Ð½Ñ– ÑÐºÑ€Ð¸Ð¿Ñ‚Ð¸ Ð· train/predict, Ð° Ð½Ðµ Ñ„ÑƒÐ½ÐºÑ†Ñ–Ñ— build_full_pipeline.
    """
    import threading
    
    # Ð¨ÑƒÐºÐ°Ñ”Ð¼Ð¾ Ñ–Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–ÑŽ Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð² ÐºÐ¾Ð´Ñ–
    # Ð¢Ð¸Ð¿Ð¾Ð²Ñ– Ð¿Ð°Ñ‚ÐµÑ€Ð½Ð¸: model = ..., clf = ..., gbdt = ...
    
    namespace = {
        'pd': pd,
        'np': np,
    }
    result = {"model": None, "error": None}
    
    def run_code():
        try:
            # Ð†Ð¼Ð¿Ð¾Ñ€Ñ‚ÑƒÑ”Ð¼Ð¾ Ð²ÑÑ– Ð¿Ð¾Ñ‚Ñ€Ñ–Ð±Ð½Ñ– Ð±Ñ–Ð±Ð»Ñ–Ð¾Ñ‚ÐµÐºÐ¸
            exec("import pandas as pd", namespace)
            exec("import numpy as np", namespace)
            exec("from sklearn.ensemble import *", namespace)
            exec("from sklearn.linear_model import *", namespace)
            exec("from sklearn.svm import *", namespace)
            exec("from sklearn.tree import *", namespace)
            exec("from sklearn.preprocessing import StandardScaler", namespace)
            exec("from sklearn.impute import SimpleImputer", namespace)
            exec("from sklearn.pipeline import Pipeline", namespace)
            
            # ÐœÐ¾Ð´Ð¸Ñ„Ñ–ÐºÑƒÑ”Ð¼Ð¾ ÐºÐ¾Ð´: Ð¿Ñ€Ð¸Ð±Ð¸Ñ€Ð°Ñ”Ð¼Ð¾ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ… Ñ‚Ð° fit
            modified_code = code
            
            # ÐŸÑ€Ð¸Ð±Ð¸Ñ€Ð°Ñ”Ð¼Ð¾ Ñ€ÑÐ´ÐºÐ¸ Ð· read_csv Ñ‚Ð° fit
            lines = modified_code.split('\n')
            filtered_lines = []
            skip_until_blank = False
            
            for line in lines:
                # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°Ñ”Ð¼Ð¾ Ð·Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÐµÐ½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ…
                if 'read_csv' in line or '.fit(' in line or 'train_test_split' in line:
                    continue
                if 'X_train' in line or 'X_val' in line or 'y_train' in line or 'y_val' in line:
                    continue
                if 'predict' in line or 'accuracy' in line or 'rmse' in line:
                    continue
                if 'print(' in line:
                    continue
                filtered_lines.append(line)
            
            modified_code = '\n'.join(filtered_lines)
            
            exec(modified_code, namespace)
            
            # Ð¨ÑƒÐºÐ°Ñ”Ð¼Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ
            model_names = ['model', 'clf', 'gbdt', 'regressor', 'classifier', 'lgb_model']
            for name in model_names:
                if name in namespace:
                    result["model"] = namespace[name]
                    return
            
            result["error"] = "ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð² ÐºÐ¾Ð´Ñ–"
            
        except Exception as e:
            result["error"] = str(e)
    
    thread = threading.Thread(target=run_code)
    thread.start()
    thread.join(timeout=10)
    
    if thread.is_alive():
        return None
    
    if result["error"]:
        print(f"  âš ï¸ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° ADK: {result['error']}")
        return None
    
    return result["model"]


def build_pipeline_from_model(model: Any, add_preprocessing: bool = True) -> Pipeline:
    """Ð¡Ñ‚Ð²Ð¾Ñ€ÑŽÑ” Pipeline Ð· Ð¼Ð¾Ð´ÐµÐ»Ñ–."""
    if add_preprocessing:
        return Pipeline([
            ('imputer', SimpleImputer(strategy='mean')),
            ('scaler', StandardScaler()),
            ('model', model)
        ])
    else:
        return Pipeline([('model', model)])


# ============================================================================
# ÐžÐ¦Ð†ÐÐ®Ð’ÐÐÐÐ¯
# ============================================================================

def evaluate_pipeline(
    pipeline: Pipeline, 
    X: np.ndarray, 
    y: np.ndarray, 
    is_classification: bool,
    n_runs: int = 5,
) -> Dict[str, float]:
    """ÐžÑ†Ñ–Ð½ÑŽÑ” pipeline Ð·Ð° Ð´Ð¾Ð¿Ð¾Ð¼Ð¾Ð³Ð¾ÑŽ cross-validation."""
    scores = []
    
    for run in range(n_runs):
        seed = 42 + run
        
        if is_classification:
            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)
            scoring = "accuracy"
        else:
            cv = KFold(n_splits=5, shuffle=True, random_state=seed)
            scoring = "r2"
        
        try:
            cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scoring, n_jobs=-1)
            scores.append(np.mean(cv_scores))
        except Exception as e:
            print(f"  âš ï¸ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° CV (run {run}): {e}")
            continue
    
    if not scores:
        return {"mean": 0.0, "std": 0.0, "n_runs": 0}
    
    return {
        "mean": np.mean(scores),
        "std": np.std(scores),
        "n_runs": len(scores),
    }


def analyze_pipeline_complexity(pipeline: Pipeline) -> Dict[str, Any]:
    """ÐÐ½Ð°Ð»Ñ–Ð·ÑƒÑ” ÑÐºÐ»Ð°Ð´Ð½Ñ–ÑÑ‚ÑŒ pipeline."""
    from sklearn.decomposition import PCA
    from sklearn.ensemble import VotingClassifier, VotingRegressor
    
    steps = pipeline.steps
    n_steps = len(steps)
    
    has_scaler = False
    has_pca = False
    has_ensemble = False
    model_type = "unknown"
    
    for name, est in steps:
        if isinstance(est, Pipeline):
            for _, nested in est.steps:
                if isinstance(nested, StandardScaler):
                    has_scaler = True
                if isinstance(nested, PCA):
                    has_pca = True
        else:
            if isinstance(est, StandardScaler):
                has_scaler = True
            if isinstance(est, PCA):
                has_pca = True
            if isinstance(est, (VotingClassifier, VotingRegressor)):
                has_ensemble = True
            if name in ["model", "estimator", "clf", "regressor"] or steps[-1] == (name, est):
                model_type = type(est).__name__
    
    return {
        "n_steps": n_steps,
        "has_scaler": has_scaler,
        "has_pca": has_pca,
        "has_ensemble": has_ensemble,
        "model_type": model_type,
    }


# ============================================================================
# ADK MLE-STAR PIPELINES
# ============================================================================

def get_adk_pipeline_for_dataset(dataset_name: str) -> Optional[Pipeline]:
    """
    Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÑƒÑ” Ð·Ð³ÐµÐ½ÐµÑ€Ð¾Ð²Ð°Ð½Ð¸Ð¹ ADK MLE-STAR pipeline Ð´Ð»Ñ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ.
    """
    # ÐœÐ°Ð¿Ð¿Ñ–Ð½Ð³ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñ–Ð² Ð½Ð° Ñ„Ð°Ð¹Ð»Ð¸ ADK pipelines
    adk_files = {
        "iris": "iris_gbdt_run1_init_code_1.py",
        "california_housing": "california-housing-prices_run1_init_code_1.py",
    }
    
    if dataset_name not in adk_files:
        print(f"  âš ï¸ ADK pipeline Ð´Ð»Ñ {dataset_name} Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾")
        return None
    
    filepath = GENERATED_PIPELINES_DIR / adk_files[dataset_name]
    
    if not filepath.exists():
        print(f"  âš ï¸ Ð¤Ð°Ð¹Ð» {filepath} Ð½Ðµ Ñ–ÑÐ½ÑƒÑ”")
        return None
    
    code = filepath.read_text(encoding='utf-8')
    
    # Ð’Ð¸Ñ‚ÑÐ³ÑƒÑ”Ð¼Ð¾ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð· ADK ÑÐºÑ€Ð¸Ð¿Ñ‚Ñƒ
    model = extract_model_from_adk_script(code)
    
    if model is None:
        # Fallback: ÑÑ‚Ð²Ð¾Ñ€ÑŽÑ”Ð¼Ð¾ pipeline Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ñ– Ñ‚Ð¸Ð¿Ñƒ Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð· ÐºÐ¾Ð´Ñƒ
        if 'HistGradientBoostingClassifier' in code:
            from sklearn.ensemble import HistGradientBoostingClassifier
            model = HistGradientBoostingClassifier(random_state=42)
        elif 'LGBMRegressor' in code or 'lightgbm' in code:
            try:
                import lightgbm as lgb
                model = lgb.LGBMRegressor(
                    objective='rmse',
                    n_estimators=100,
                    learning_rate=0.1,
                    random_state=42
                )
            except ImportError:
                from sklearn.ensemble import GradientBoostingRegressor
                model = GradientBoostingRegressor(random_state=42)
        elif 'RandomForestClassifier' in code:
            from sklearn.ensemble import RandomForestClassifier
            model = RandomForestClassifier(random_state=42)
        else:
            print(f"  âš ï¸ ÐÐµ Ð²Ð´Ð°Ð»Ð¾ÑÑ Ð²Ð¸Ð·Ð½Ð°Ñ‡Ð¸Ñ‚Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð· ADK ÐºÐ¾Ð´Ñƒ")
            return None
    
    return build_pipeline_from_model(model)


# ============================================================================
# Ð•ÐšÐ¡ÐŸÐ•Ð Ð˜ÐœÐ•ÐÐ¢Ð˜
# ============================================================================

def run_prompt_experiment(
    dataset_name: str,
    prompt_type: str,  # "simple" Ð°Ð±Ð¾ "mle_star"
    llm_type: str,     # "gemini" Ð°Ð±Ð¾ "gpt4o"
    n_runs: int = 5,
) -> Dict[str, Any]:
    """Ð—Ð°Ð¿ÑƒÑÐºÐ°Ñ” ÐµÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚ Ð· Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð¼."""
    print(f"\n{'='*60}")
    print(f"Ð”Ð°Ñ‚Ð°ÑÐµÑ‚: {dataset_name} | ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚: {prompt_type} | LLM: {llm_type}")
    print(f"{'='*60}")
    
    info = get_dataset_info(dataset_name)
    
    if prompt_type == "simple":
        template = SIMPLE_PROMPT_TEMPLATE
    else:
        template = MLE_STAR_PROMPT_TEMPLATE
    
    prompt = format_prompt(template, info)
    
    print(f"ðŸ“¤ Ð’Ñ–Ð´Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ Ð·Ð°Ð¿Ð¸Ñ‚ Ð´Ð¾ {llm_type}...")
    start_time = time.time()
    
    try:
        if llm_type == "gemini":
            response = call_gemini(prompt)
        else:
            response = call_openai(prompt)
        
        api_time = time.time() - start_time
        print(f"âœ… Ð’Ñ–Ð´Ð¿Ð¾Ð²Ñ–Ð´ÑŒ Ð¾Ñ‚Ñ€Ð¸Ð¼Ð°Ð½Ð¾ Ð·Ð° {api_time:.1f}Ñ")
        
    except Exception as e:
        print(f"âŒ ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ° API: {e}")
        return {
            "dataset": dataset_name,
            "prompt_type": prompt_type,
            "llm_type": llm_type,
            "generation_method": f"{prompt_type}_{llm_type}",
            "status": "api_error",
            "error": str(e),
        }
    
    code = extract_code(response)
    
    print("ðŸ”§ Ð‘ÑƒÐ´ÑƒÑŽ pipeline...")
    pipeline = execute_code_and_get_pipeline(code)
    
    if pipeline is None:
        return {
            "dataset": dataset_name,
            "prompt_type": prompt_type,
            "llm_type": llm_type,
            "generation_method": f"{prompt_type}_{llm_type}",
            "status": "code_error",
            "code": code[:500],
        }
    
    print(f"ðŸ“Š ÐžÑ†Ñ–Ð½ÑŽÑŽ pipeline ({n_runs} Ð·Ð°Ð¿ÑƒÑÐºÑ–Ð²)...")
    metrics = evaluate_pipeline(
        pipeline, 
        info["X"], 
        info["y"], 
        info["is_classification"],
        n_runs=n_runs,
    )
    
    print(f"âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {metrics['mean']:.4f} Â± {metrics['std']:.4f}")
    
    pipeline_info = analyze_pipeline_complexity(pipeline)
    
    return {
        "dataset": dataset_name,
        "prompt_type": prompt_type,
        "llm_type": llm_type,
        "generation_method": f"{prompt_type}_{llm_type}",
        "status": "success",
        "score_mean": metrics["mean"],
        "score_std": metrics["std"],
        "n_runs": metrics["n_runs"],
        "api_time_sec": api_time,
        **pipeline_info,
    }


def run_adk_experiment(
    dataset_name: str,
    n_runs: int = 5,
) -> Dict[str, Any]:
    """Ð—Ð°Ð¿ÑƒÑÐºÐ°Ñ” ÐµÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚ Ð· ADK MLE-STAR pipeline."""
    print(f"\n{'='*60}")
    print(f"Ð”Ð°Ñ‚Ð°ÑÐµÑ‚: {dataset_name} | ÐœÐµÑ‚Ð¾Ð´: ADK MLE-STAR Agent")
    print(f"{'='*60}")
    
    info = get_dataset_info(dataset_name)
    
    print("ðŸ”§ Ð—Ð°Ð²Ð°Ð½Ñ‚Ð°Ð¶ÑƒÑŽ ADK pipeline...")
    pipeline = get_adk_pipeline_for_dataset(dataset_name)
    
    if pipeline is None:
        return {
            "dataset": dataset_name,
            "prompt_type": "adk_agent",
            "llm_type": "gemini_adk",
            "generation_method": "adk_mle_star",
            "status": "not_available",
        }
    
    print(f"ðŸ“Š ÐžÑ†Ñ–Ð½ÑŽÑŽ pipeline ({n_runs} Ð·Ð°Ð¿ÑƒÑÐºÑ–Ð²)...")
    metrics = evaluate_pipeline(
        pipeline, 
        info["X"], 
        info["y"], 
        info["is_classification"],
        n_runs=n_runs,
    )
    
    print(f"âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚: {metrics['mean']:.4f} Â± {metrics['std']:.4f}")
    
    pipeline_info = analyze_pipeline_complexity(pipeline)
    
    return {
        "dataset": dataset_name,
        "prompt_type": "adk_agent",
        "llm_type": "gemini_adk",
        "generation_method": "adk_mle_star",
        "status": "success",
        "score_mean": metrics["mean"],
        "score_std": metrics["std"],
        "n_runs": metrics["n_runs"],
        "api_time_sec": 0,  # Pre-generated
        **pipeline_info,
    }


def run_full_experiment():
    """Ð—Ð°Ð¿ÑƒÑÐºÐ°Ñ” Ð¿Ð¾Ð²Ð½Ð¸Ð¹ ÐºÐ¾Ð¼Ð¿Ð»ÐµÐºÑÐ½Ð¸Ð¹ ÐµÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚."""
    print("\n" + "="*70)
    print("ðŸ§ª ÐšÐžÐœÐŸÐ›Ð•ÐšÐ¡ÐÐ˜Ð™ Ð•ÐšÐ¡ÐŸÐ•Ð Ð˜ÐœÐ•ÐÐ¢: ÐŸÐžÐ Ð†Ð’ÐÐ¯ÐÐÐ¯ ÐœÐ•Ð¢ÐžÐ”Ð†Ð’ Ð“Ð•ÐÐ•Ð ÐÐ¦Ð†Ð‡ ML PIPELINES")
    print("="*70)
    
    if not GEMINI_API_KEY:
        print("âŒ GEMINI_API_TOKEN Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾!")
        return
    if not OPENAI_API_KEY:
        print("âš ï¸ OPENAI_API_KEY Ð½Ðµ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ - GPT-4o Ð±ÑƒÐ´Ðµ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾")
    
    datasets = ["iris", "breast_cancer", "california_housing"]
    prompt_types = ["simple", "mle_star"]
    llm_types = ["gemini"]
    if OPENAI_API_KEY:
        llm_types.append("gpt4o")
    
    results = []
    
    # 1. ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð²Ñ– ÐµÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¸
    for dataset in datasets:
        for prompt_type in prompt_types:
            for llm_type in llm_types:
                result = run_prompt_experiment(
                    dataset_name=dataset,
                    prompt_type=prompt_type,
                    llm_type=llm_type,
                    n_runs=5,
                )
                results.append(result)
                time.sleep(1)
    
    # 2. ADK MLE-STAR ÐµÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð¸
    for dataset in datasets:
        result = run_adk_experiment(dataset_name=dataset, n_runs=5)
        results.append(result)
    
    # Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸
    df = pd.DataFrame(results)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    csv_path = RESULTS_DIR / f"comprehensive_experiment_{timestamp}.csv"
    df.to_csv(csv_path, index=False)
    print(f"\nðŸ“ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸ Ð·Ð±ÐµÑ€ÐµÐ¶ÐµÐ½Ð¾: {csv_path}")
    
    # Ð’Ð¸Ð²Ð¾Ð´Ð¸Ð¼Ð¾ Ð·Ð²ÐµÐ´ÐµÐ½Ñƒ Ñ‚Ð°Ð±Ð»Ð¸Ñ†ÑŽ
    print("\n" + "="*70)
    print("ðŸ“Š Ð—Ð’Ð•Ð”Ð•ÐÐ† Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð˜")
    print("="*70)
    
    success_df = df[df["status"] == "success"]
    if len(success_df) > 0:
        summary = success_df.groupby(["dataset", "generation_method"]).agg({
            "score_mean": "mean",
            "score_std": "mean",
            "n_steps": "mean",
            "has_pca": "mean",
            "has_ensemble": "mean",
            "model_type": "first",
        }).round(4)
        print(summary.to_string())
    
    # ÐŸÐ¾Ñ€Ñ–Ð²Ð½ÑÐ½Ð½Ñ Ð¼ÐµÑ‚Ð¾Ð´Ñ–Ð²
    print("\n" + "="*70)
    print("ðŸ“ˆ ÐŸÐžÐ Ð†Ð’ÐÐ¯ÐÐÐ¯ ÐœÐ•Ð¢ÐžÐ”Ð†Ð’ Ð“Ð•ÐÐ•Ð ÐÐ¦Ð†Ð‡")
    print("="*70)
    
    if len(success_df) > 0:
        method_summary = success_df.groupby("generation_method").agg({
            "score_mean": ["mean", "std"],
            "n_steps": "mean",
        }).round(4)
        print(method_summary.to_string())
    
    return df


if __name__ == "__main__":
    run_full_experiment()
