#!/usr/bin/env python
"""
–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Ä—ñ–∑–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π Gemini –Ω–∞ –≤—Å—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö.
–ì–µ–Ω–µ—Ä—É—î–º–æ pipeline –¥–ª—è –∫–æ–∂–Ω–æ—ó –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—ó (–º–æ–¥–µ–ª—å √ó –¥–∞—Ç–∞—Å–µ—Ç) —Ç–∞ –ø–æ—Ä—ñ–≤–Ω—é—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏.

–ê–≤—Ç–æ—Ä: –§–µ—Ñ–µ–ª–æ–≤ –Ü–ª–ª—è –û–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–∏—á
–ú–ê–£–ü, 2025
"""

import os
import sys
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Tuple, List

import google.generativeai as genai
import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer, load_wine, load_digits, load_iris
from sklearn.model_selection import cross_val_score

# –î–æ–¥–∞—î–º–æ src –¥–æ —à–ª—è—Ö—É
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.mle_star_ablation.datasets import DatasetLoader
from src.mle_star_ablation.prompts import generate_mle_prompt


# –ú–æ–¥–µ–ª—ñ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
GEMINI_MODELS = [
    "gemini-2.5-flash-lite",    # –ù–∞–π—à–≤–∏–¥—à–∞
    "gemini-2.5-flash",          # –ó–±–∞–ª–∞–Ω—Å–æ–≤–∞–Ω–∞
    "gemini-2.5-pro",            # –ù–∞–π—Ä–æ–∑—É–º–Ω—ñ—à–∞
]

# –î–∞—Ç–∞—Å–µ—Ç–∏ –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
DATASETS = ["breast_cancer", "wine", "digits", "iris"]


def generate_pipeline_code(model_name: str, dataset_name: str, api_key: str) -> Tuple[str, Dict]:
    """–ì–µ–Ω–µ—Ä—É—î ML pipeline —á–µ—Ä–µ–∑ Gemini API."""
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(model_name)
    
    dataset_info = DatasetLoader.get_dataset_info(dataset_name)
    
    prompt = generate_mle_prompt(dataset_name, dataset_info)
    
    start_time = time.time()
    # Deterministic generation parameters if supported (fallback)
    try:
        response = model.generate_content(prompt, temperature=0, top_p=1)
    except TypeError:
        response = model.generate_content(prompt)
    generation_time = time.time() - start_time
    
    code = response.text
    if "```python" in code:
        code = code.split("```python")[1].split("```")[0].strip()
    elif "```" in code:
        code = code.split("```")[1].split("```")[0].strip()
    
    metadata = {
        "model": model_name,
        "dataset": dataset_name,
        "generation_time": generation_time,
        "code_length": len(code),
        "timestamp": datetime.now().isoformat()
    }
    
    return code, metadata


def test_pipeline(code: str, dataset_name: str) -> Dict:
    """–¢–µ—Å—Ç—É—î –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π pipeline."""
    try:
        # –í–∏–∫–æ–Ω—É—î–º–æ –∫–æ–¥
        namespace = {}
        exec(code, namespace)
        
        if 'build_full_pipeline' not in namespace:
            return {"error": "Function 'build_full_pipeline' not found", "success": False}
        
        build_full_pipeline = namespace['build_full_pipeline']
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –¥–∞–Ω—ñ
        if dataset_name == "breast_cancer":
            X, y = load_breast_cancer(return_X_y=True)
        elif dataset_name == "wine":
            X, y = load_wine(return_X_y=True)
        elif dataset_name == "digits":
            X, y = load_digits(return_X_y=True)
        elif dataset_name == "iris":
            X, y = load_iris(return_X_y=True)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ pipeline
        pipeline = build_full_pipeline()
        
        # –¢–µ—Å—Ç—É—î–º–æ –∑ full cross-validation
        scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')
        
        # –í–∏–∑–Ω–∞—á–∞—î–º–æ –º–æ–¥–µ–ª—å
        model_name = type(pipeline.named_steps['model']).__name__
        
        return {
            "accuracy_mean": float(scores.mean()),
            "accuracy_std": float(scores.std()),
            "accuracy_min": float(scores.min()),
            "accuracy_max": float(scores.max()),
            "model_used": model_name,
            "pipeline_steps": [name for name, _ in pipeline.steps],
            "success": True
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "success": False
        }


def analyze_model_choice(code: str) -> Dict:
    """–ê–Ω–∞–ª—ñ–∑—É—î —è–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—Ä–∞–ª–∞ –º–æ–¥–µ–ª—å."""
    model_keywords = {
        "LogisticRegression": "Logistic Regression",
        "RandomForest": "Random Forest",
        "GradientBoosting": "Gradient Boosting",
        "SVC": "Support Vector Machine",
        "MLPClassifier": "Neural Network (MLP)",
        "DecisionTree": "Decision Tree",
        "KNeighbors": "K-Nearest Neighbors",
        "GaussianNB": "Naive Bayes",
    }
    
    detected_models = []
    for keyword, full_name in model_keywords.items():
        if keyword in code:
            detected_models.append(full_name)
    
    return {
        "detected_models": detected_models,
        "complexity": "complex" if any(m in detected_models for m in 
            ["Random Forest", "Gradient Boosting", "Neural Network (MLP)"]) else "simple"
    }


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--api-key', type=str, help='Gemini API Key')
    parser.add_argument('--datasets', nargs='*', default=None, help='List of datasets to run (default: all)')
    args = parser.parse_args()
    
    api_key = args.api_key or os.getenv('GEMINI_API_KEY')
    if not api_key:
        # Try loading from .env
        from pathlib import Path
        env_path = Path('.env')
        if env_path.exists():
            with open(env_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if 'GEMINI_API_KEY=' in line:
                        api_key = line.strip().split('=', 1)[1]
                        os.environ['GEMINI_API_KEY'] = api_key
                        break
    
    if not api_key:
        print("‚ùå Error: GEMINI_API_KEY not set!")
        print("Usage: python compare_gemini_models_on_datasets.py --api-key YOUR_KEY")
        return
    
    datasets_to_run = args.datasets if args.datasets else DATASETS
    
    output_dir = Path("model_comparison_results")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    print("="*80)
    print("COMPREHENSIVE GEMINI MODEL COMPARISON")
    print("="*80)
    print(f"Models: {', '.join(GEMINI_MODELS)}")
    print(f"Datasets: {', '.join(datasets_to_run)}")
    print(f"Total combinations: {len(GEMINI_MODELS)} √ó {len(datasets_to_run)} = {len(GEMINI_MODELS) * len(datasets_to_run)}")
    print("="*80)
    
    all_results = []
    total_experiments = len(GEMINI_MODELS) * len(datasets_to_run)
    experiment_num = 0
    
    for dataset_name in datasets_to_run:
        print(f"\n{'='*80}")
        print(f"DATASET: {dataset_name.upper()}")
        print(f"{'='*80}")
        
        for model_name in GEMINI_MODELS:
            experiment_num += 1
            print(f"\n[{experiment_num}/{total_experiments}] Testing: {model_name}")
            
            # –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è pipeline
            print(f"   ü§ñ Generating pipeline...", end=" ")
            try:
                code, metadata = generate_pipeline_code(model_name, dataset_name, api_key)
                print(f"‚úì ({metadata['generation_time']:.2f}s)")
                
                # –ê–Ω–∞–ª—ñ–∑ –≤–∏–±–æ—Ä—É –º–æ–¥–µ–ª—ñ
                model_analysis = analyze_model_choice(code)
                print(f"   üîç Detected models: {', '.join(model_analysis['detected_models'])}")
                
                # –¢–µ—Å—Ç—É–≤–∞–Ω–Ω—è pipeline
                print(f"   üß™ Testing pipeline...", end=" ")
                test_results = test_pipeline(code, dataset_name)
                
                if test_results['success']:
                    print(f"‚úì Accuracy: {test_results['accuracy_mean']:.4f} ¬± {test_results['accuracy_std']:.4f}")
                    print(f"   üìä Model used: {test_results['model_used']}")
                else:
                    print(f"‚úó Error: {test_results['error'][:80]}")
                
                # –ó–±–∏—Ä–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
                result = {
                    **metadata,
                    **test_results,
                    **model_analysis
                }
                all_results.append(result)
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–æ–¥
                code_file = output_dir / f"{model_name.replace('-', '_')}_{dataset_name}.py"
                with open(code_file, 'w', encoding='utf-8') as f:
                    f.write(f'"""\nModel: {model_name}\nDataset: {dataset_name}\n')
                    f.write(f'Generated: {metadata["timestamp"]}\n"""\n\n{code}')
                
                # –ü–∞—É–∑–∞ –¥–ª—è rate limit
                if experiment_num < total_experiments:
                    wait_time = 35 if "pro" in model_name else 5
                    print(f"   ‚è≥ Waiting {wait_time}s...")
                    time.sleep(wait_time)
                    
            except Exception as e:
                print(f"‚úó Error: {str(e)[:100]}")
                all_results.append({
                    "model": model_name,
                    "dataset": dataset_name,
                    "error": str(e),
                    "success": False
                })
    
    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    print(f"\n{'='*80}")
    print("SAVING RESULTS")
    print(f"{'='*80}")
    
    # JSON
    json_file = output_dir / f"comparison_full_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ JSON: {json_file}")
    
    # CSV
    df = pd.DataFrame(all_results)
    csv_file = output_dir / f"comparison_full_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(csv_file, index=False)
    print(f"‚úÖ CSV: {csv_file}")
    
    # –ü—ñ–¥—Å—É–º–∫–æ–≤–∞ —Ç–∞–±–ª–∏—Ü—è
    print(f"\n{'='*80}")
    print("SUMMARY RESULTS")
    print(f"{'='*80}\n")
    
    # –£—Å–ø—ñ—à–Ω—ñ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏
    successful = df[df['success'] == True]
    
    if len(successful) > 0:
        # –¢–∞–±–ª–∏—Ü—è accuracy –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º
        print("Accuracy by Dataset and Model:")
        print("-"*80)
        pivot = successful.pivot_table(
            values='accuracy_mean',
            index='model',
            columns='dataset',
            aggfunc='mean'
        )
        print(pivot.to_string())
        
        # –°–µ—Ä–µ–¥–Ω—è accuracy –ø–æ –º–æ–¥–µ–ª—è—Ö
        print(f"\n\nAverage Accuracy by Model:")
        print("-"*80)
        avg_by_model = successful.groupby('model')['accuracy_mean'].agg(['mean', 'std', 'count'])
        print(avg_by_model.to_string())
        
        # –°–µ—Ä–µ–¥–Ω—è —à–≤–∏–¥–∫—ñ—Å—Ç—å –ø–æ –º–æ–¥–µ–ª—è—Ö
        print(f"\n\nGeneration Time by Model:")
        print("-"*80)
        time_by_model = successful.groupby('model')['generation_time'].agg(['mean', 'min', 'max'])
        print(time_by_model.to_string())
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏
        print(f"\n\nModel Algorithms Used:")
        print("-"*80)
        algo_counts = successful['model_used'].value_counts()
        print(algo_counts.to_string())
        
        # –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
        print(f"\n\nBest Model per Dataset:")
        print("-"*80)
        best_per_dataset = successful.loc[successful.groupby('dataset')['accuracy_mean'].idxmax()]
        for _, row in best_per_dataset.iterrows():
            print(f"  {row['dataset']:15s} ‚Üí {row['model']:25s} "
                  f"({row['accuracy_mean']:.4f}, {row['model_used']})")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó
        print(f"\n{'='*80}")
        print("RECOMMENDATIONS")
        print(f"{'='*80}")
        
        best_overall = successful.loc[successful['accuracy_mean'].idxmax()]
        fastest = successful.loc[successful['generation_time'].idxmin()]
        
        print(f"\nüéØ Best Overall Performance:")
        print(f"   Model: {best_overall['model']}")
        print(f"   Dataset: {best_overall['dataset']}")
        print(f"   Accuracy: {best_overall['accuracy_mean']:.4f}")
        print(f"   Algorithm: {best_overall['model_used']}")
        
        print(f"\n‚ö° Fastest Generation:")
        print(f"   Model: {fastest['model']}")
        print(f"   Time: {fastest['generation_time']:.2f}s")
        print(f"   Accuracy: {fastest['accuracy_mean']:.4f}")
        
        # –ê–Ω–∞–ª—ñ–∑ —Å–∫–ª–∞–¥–Ω–æ—Å—Ç—ñ –∞–ª–≥–æ—Ä–∏—Ç–º—ñ–≤
        complex_count = len(successful[successful['complexity'] == 'complex'])
        simple_count = len(successful[successful['complexity'] == 'simple'])
        
        print(f"\nüß† Algorithm Complexity:")
        print(f"   Complex models (RF/GB/MLP): {complex_count}/{len(successful)} ({complex_count/len(successful)*100:.1f}%)")
        print(f"   Simple models (LR): {simple_count}/{len(successful)} ({simple_count/len(successful)*100:.1f}%)")
        
        if complex_count > 0:
            complex_acc = successful[successful['complexity'] == 'complex']['accuracy_mean'].mean()
            simple_acc = successful[successful['complexity'] == 'simple']['accuracy_mean'].mean() if simple_count > 0 else 0
            print(f"   Avg accuracy (complex): {complex_acc:.4f}")
            if simple_count > 0:
                print(f"   Avg accuracy (simple): {simple_acc:.4f}")
                print(f"   Difference: {complex_acc - simple_acc:+.4f}")
    
    print(f"\n{'='*80}")


if __name__ == "__main__":
    main()
