#!/usr/bin/env python
"""
Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¸Ğ¹ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ ĞµĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ–Ğ²:
1. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ ML pipeline Ñ‡ĞµÑ€ĞµĞ· Gemini API Ğ´Ğ»Ñ ĞºĞ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ
2. Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ğ·Ğ³ĞµĞ½ĞµÑ€Ğ¾Ğ²Ğ°Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ñƒ
3. ĞŸÑ€Ğ¾Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ ablation Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ
4. Ğ—Ğ±Ñ–Ñ€ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ–Ğ² Ğ´Ğ»Ñ Ğ²ÑÑ–Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ–Ğ²

ĞĞ²Ñ‚Ğ¾Ñ€: Ğ¤ĞµÑ„ĞµĞ»Ğ¾Ğ² Ğ†Ğ»Ğ»Ñ ĞĞ»ĞµĞºÑĞ°Ğ½Ğ´Ñ€Ğ¾Ğ²Ğ¸Ñ‡
ĞœĞĞ£ĞŸ, 2025
"""

import os
import sys
import argparse
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List

import google.generativeai as genai
import numpy as np
import pandas as pd

# Ğ”Ğ¾Ğ´Ğ°Ñ”Ğ¼Ğ¾ src Ğ´Ğ¾ ÑˆĞ»ÑÑ…Ñƒ
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.mle_star_ablation.config import get_standard_configs
from src.mle_star_ablation.datasets import DatasetLoader
from src.mle_star_ablation.metrics import calculate_classification_metrics
from src.mle_star_ablation.ast_utils import inject_random_state_into_build_fn
from src.mle_star_ablation.prompts import PromptBuilder
from scripts.validate_generated_pipeline import validate_pipeline_file


def generate_pipeline_with_gemini(dataset_name: str, api_key: str) -> str:
    """
    Ğ“ĞµĞ½ĞµÑ€ÑƒÑ” ML pipeline Ñ‡ĞµÑ€ĞµĞ· Gemini API Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ.
    
    parser.add_argument(
        '--seed',
        type=int,
        default=None,
        help='Optional base seed for deterministic runs'
    )
    parser.add_argument(
        '--deterministic',
        action='store_true',
        help='Attempt to enforce deterministic runs (set threads to 1, seed RNGs)'
    )
    Args:
        dataset_name: ĞĞ°Ğ·Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ (breast_cancer, wine, digits, iris)
        api_key: Gemini API key
        
    Returns:
        str: Ğ—Ğ³ĞµĞ½ĞµÑ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Python ĞºĞ¾Ğ´ pipeline
    """
    # ĞšĞ¾Ğ½Ñ„Ñ–Ğ³ÑƒÑ€Ğ°Ñ†Ñ–Ñ Gemini - Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ”Ğ¼Ğ¾ Ğ½Ğ°Ğ¹ÑˆĞ²Ğ¸Ğ´ÑˆÑƒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ· paid tier
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.5-flash-lite")
    
    # ĞÑ‚Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ½Ñ Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ñ— Ğ¿Ñ€Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚
    dataset_info = DatasetLoader.get_dataset_info(dataset_name)
    
    # ĞŸÑ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ—
    builder = PromptBuilder(dataset_name, dataset_info)
    builder.add_role_context()
    builder.add_task_description()
    builder.add_dataset_info()
    builder.add_requirements()
    builder.add_output_format()
    builder.add_constraints()
    
    prompt = builder.build()
    
    print(f"\nğŸ“¡ Generating pipeline for {dataset_name} via Gemini API (deterministic generation: temperature=0, top_p=1)...")
    # Force deterministic generation (temperature=0) to improve reproducibility
    response = model.generate_content(prompt, temperature=0, top_p=1)
    
    # Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ”Ğ¼Ğ¾ ĞºĞ¾Ğ´ Ğ· Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–
    code = response.text
    
    # Ğ¯ĞºÑ‰Ğ¾ ĞºĞ¾Ğ´ Ğ¾Ğ±Ğ³Ğ¾Ñ€Ğ½ÑƒÑ‚Ğ¸Ğ¹ Ñƒ ```python, Ñ€Ğ¾Ğ·Ğ¿Ğ°ĞºĞ¾Ğ²ÑƒÑ”Ğ¼Ğ¾
    if "```python" in code:
        code = code.split("```python")[1].split("```")[0].strip()
    elif "```" in code:
        code = code.split("```")[1].split("```")[0].strip()
    # Also save the full raw response for auditability
    try:
        out_dir = Path('generated_pipelines')
        out_dir.mkdir(parents=True, exist_ok=True)
        raw_file = out_dir / f"pipeline_{dataset_name}_raw_response.txt"
        with open(raw_file, 'w', encoding='utf-8') as rf:
            rf.write(response.text)
    except Exception:
        pass
    
    return code


def save_generated_pipeline(code: str, dataset_name: str, output_dir: Path):
    """
    Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ” Ğ·Ğ³ĞµĞ½ĞµÑ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ´ Ñƒ Ñ„Ğ°Ğ¹Ğ».
    
    Args:
        code: Ğ—Ğ³ĞµĞ½ĞµÑ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ Python ĞºĞ¾Ğ´
        dataset_name: ĞĞ°Ğ·Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ
        output_dir: Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ñ–Ñ Ğ´Ğ»Ñ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ
    """
    output_dir.mkdir(exist_ok=True, parents=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = output_dir / f"pipeline_{dataset_name}_{timestamp}.py"
    
    # Ğ¤Ğ¾Ñ€Ğ¼ÑƒÑ”Ğ¼Ğ¾ Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ» Ğ· header
    full_code = f'''"""
Generated ML Pipeline for {dataset_name} dataset
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Generator: Google Gemini 2.0 Flash Exp
"""

{code}


if __name__ == "__main__":
    # Test the pipeline
    from sklearn.datasets import load_{dataset_name}
    from sklearn.model_selection import cross_val_score
    
    X, y = load_{dataset_name}(return_X_y=True)
    pipeline = build_full_pipeline(random_state=42)
    
    scores = cross_val_score(pipeline, X, y, cv=3, scoring='accuracy')
    print(f"Pipeline for {dataset_name}:")
    print(f"  Accuracy: {{scores.mean():.3f}} Â± {{scores.std():.3f}}")
'''
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(full_code)
    
    print(f"âœ… Saved to {filename}")
    return filename


def update_mle_star_pipeline(code: str, dataset_name: str, generated_file: Path | None = None):
    """
    ĞĞ½Ğ¾Ğ²Ğ»ÑÑ” mle_star_generated_pipeline.py Ğ½Ğ¾Ğ²Ğ¸Ğ¼ ĞºĞ¾Ğ´Ğ¾Ğ¼.
    
    Args:
        code: Ğ—Ğ³ĞµĞ½ĞµÑ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ´ pipeline
        dataset_name: ĞĞ°Ğ·Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ (Ğ´Ğ»Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ñ–Ñ—)
    """
    pipeline_file = Path(__file__).parent.parent / "src" / "mle_star_ablation" / "mle_star_generated_pipeline.py"
    
    # Ğ§Ğ¸Ñ‚Ğ°Ñ”Ğ¼Ğ¾ Ğ¿Ğ¾Ñ‚Ğ¾Ñ‡Ğ½Ğ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ»
    with open(pipeline_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ğ—Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ ÑĞµĞºÑ†Ñ–Ñ build_full_pipeline
    start_marker = "def build_full_pipeline("
    end_marker = "\n\n# ==== 2. ĞšĞĞĞ¤Ğ†Ğ“Ğ£Ğ ĞĞ¦Ğ†Ğ¯ ĞĞĞ—Ğ’ ĞšĞ ĞĞšĞ†Ğ’"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("Could not find build_full_pipeline section in the file")
    
    # Ğ¤Ğ¾Ñ€Ğ¼ÑƒÑ”Ğ¼Ğ¾ Ğ½Ğ¾Ğ²Ğ¸Ğ¹ ĞºĞ¾Ğ´ Ğ· Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¸Ğ¼ build_full_pipeline Ñ‚Ğ° Ğ´Ğ¾Ğ´Ğ°Ñ‚ĞºĞ¾Ğ²Ğ¾Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¾Ñ random_state
    try:
        fixed_code = inject_random_state_into_build_fn(code)
    except Exception as e:
        print(f"âš ï¸ AST transform failed: {e}; falling back to basic replacement")
        fixed_code = code.replace("def build_full_pipeline():", "def build_full_pipeline(random_state: int = 42):")
        fixed_code = fixed_code.replace('random_state=42', 'random_state=random_state')
    # fixed_code contains build_full_pipeline function body with random_state injection
    
    # Before replacing pipeline, validate the generated file (if available)
    if generated_file is not None and generated_file.exists():
        ok = validate_pipeline_file(generated_file, cv=2, random_state=42)
        if not ok:
            print(f"âŒ Validation of generated pipeline {generated_file} failed. Skipping update.")
            return

    # Ğ—Ğ°Ğ¼Ñ–Ğ½ÑÑ”Ğ¼Ğ¾ ÑÑ‚Ğ°Ñ€Ğ¸Ğ¹ ĞºĞ¾Ğ´ Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ— build_full_pipeline Ñƒ Ñ„Ğ°Ğ¹Ğ»Ñ–
    new_content = content[:start_idx] + fixed_code + content[end_idx:]
    
    # Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾
    with open(pipeline_file, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    # Reload and validate the new pipeline module to ensure it exposes a usable build_full_pipeline
    try:
        import importlib
        import src.mle_star_ablation.mle_star_generated_pipeline as mgp
        importlib.reload(mgp)
        # call build_full_pipeline to ensure it returns a pipeline-like object
        pipeline_instance = mgp.build_full_pipeline(random_state=42)
        info = mgp.inspect_pipeline(pipeline_instance)
        if 'model' not in info['steps']:
            print(f"âš ï¸ Validation: model step not found in pipeline steps: {info['steps']} (skipping update)")
            return
    except Exception as e:
        print(f"âš ï¸ Could not import/validate updated mle_star_generated_pipeline: {e}")
        return

    print(f"âœ… Updated and validated {pipeline_file}")


def run_ablation_for_dataset(dataset_name: str, n_runs: int = 5, base_seed: int = None, no_plots: bool = False, deterministic: bool = False, forced_task_type: str = None) -> Dict:
    """
    Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°Ñ” ablation Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ.
    
    Args:
        dataset_name: ĞĞ°Ğ·Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ
        n_runs: ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ñ–Ğ² ĞºĞ¾Ğ¶Ğ½Ğ¾Ñ— ĞºĞ¾Ğ½Ñ„Ñ–Ğ³ÑƒÑ€Ğ°Ñ†Ñ–Ñ—
        
    Returns:
        Dict: Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸ Ğ²ÑÑ–Ñ… ĞºĞ¾Ğ½Ñ„Ñ–Ğ³ÑƒÑ€Ğ°Ñ†Ñ–Ğ¹
    """
    from scripts.run_ablation import run_multiple_runs
    
    configs = get_standard_configs()
    results = {}
    
    print(f"\nğŸ”¬ Running ablation analysis for {dataset_name}...")
    print(f"   Configurations: {len(configs)}")
    print(f"   Runs per config: {n_runs}")
    
    for idx, config in enumerate(configs, 1):
        config_name = config.get_name()
        print(f"   [{idx}/{len(configs)}] {config_name}...", end=" ")
        
        try:
            config_results = run_multiple_runs(
                config, dataset_name, None, None, n_runs, base_seed=base_seed, deterministic=deterministic, forced_task_type=forced_task_type
            )
            results[config_name] = config_results
            
            avg_acc = np.mean([r['accuracy'] for r in config_results])
            print(f"âœ“ {avg_acc:.4f}")
            
        except Exception as e:
            print(f"âœ— Error: {e}")
            results[config_name] = []
    
    return results


def main():
    parser = argparse.ArgumentParser(
        description='ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ Ñ‚Ğ° ablation Ğ°Ğ½Ğ°Ğ»Ñ–Ğ· Ğ´Ğ»Ñ Ğ²ÑÑ–Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ–Ğ²',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        '--datasets',
        type=str,
        nargs='+',
        default=['breast_cancer', 'wine', 'digits', 'iris'],
        help='Ğ¡Ğ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ–Ğ² Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ'
    )
    parser.add_argument(
        '--n-runs',
        type=int,
        default=5,
        help='ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ñ–Ğ² ĞºĞ¾Ğ¶Ğ½Ğ¾Ñ— ĞºĞ¾Ğ½Ñ„Ñ–Ğ³ÑƒÑ€Ğ°Ñ†Ñ–Ñ—'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='results_full_experiment',
        help='Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ñ–Ñ Ğ´Ğ»Ñ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ–Ğ²'
    )
    parser.add_argument(
        '--skip-generation',
        action='store_true',
        help='ĞŸÑ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ pipeline (Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ°Ñ‚Ğ¸ Ñ–ÑĞ½ÑƒÑÑ‡Ğ¸Ğ¹)'
    )
    parser.add_argument(
        '--api-key',
        type=str,
        default=None,
        help='Gemini API key (Ğ°Ğ±Ğ¾ Ñ‡ĞµÑ€ĞµĞ· GEMINI_API_KEY env)'
    )
    parser.add_argument(
        '--seed',
        type=int,
        default=None,
        help='Optional base seed for deterministic runs'
    )
    parser.add_argument(
        '--deterministic',
        action='store_true',
        help='Attempt to enforce deterministic runs (set threads to 1, seed RNGs)'
    )
    parser.add_argument(
        '--no-plots',
        action='store_true',
        help='Do not create plots while running main_experiment'
    )
    parser.add_argument(
        '--task-type',
        type=str,
        choices=['classification', 'regression'],
        default=None,
        help='Force the task type for run_ablation_for_dataset (overrides automatic detection)'
    )
    
    args = parser.parse_args()
    
    # API key
    api_key = args.api_key or os.getenv('GEMINI_API_KEY')
    if not api_key and not args.skip_generation:
        raise ValueError("Gemini API key required! Set --api-key or GEMINI_API_KEY env")
    
    # Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ”Ğ¼Ğ¾ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ñ–Ñ—
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    generated_dir = Path("generated_pipelines")
    
    print("="*80)
    print("AUTOMATED ABLATION EXPERIMENT")
    print("="*80)
    print(f"Datasets: {', '.join(args.datasets)}")
    print(f"Runs per config: {args.n_runs}")
    print(f"Output: {args.output_dir}")
    print("="*80)
    
    # Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸ Ğ´Ğ»Ñ Ğ²ÑÑ–Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñ–Ğ²
    all_dataset_results = {}
    
    for dataset_name in args.datasets:
        print(f"\n{'='*80}")
        print(f"DATASET: {dataset_name.upper()}")
        print(f"{'='*80}")
        
        # 1. Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ñ–Ñ pipeline (ÑĞºÑ‰Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾)
        if not args.skip_generation:
            try:
                code = generate_pipeline_with_gemini(dataset_name, api_key)
                
                # Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾ Ğ·Ğ³ĞµĞ½ĞµÑ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ´
                saved = save_generated_pipeline(code, dataset_name, generated_dir)
                
                # ĞĞ½Ğ¾Ğ²Ğ»ÑÑ”Ğ¼Ğ¾ mle_star_generated_pipeline.py (preflight validation occurs here)
                update_mle_star_pipeline(code, dataset_name, generated_file=saved)
                
                print("âœ… Pipeline generated and installed")
                
            except Exception as e:
                print(f"âŒ Generation failed: {e}")
                continue
        else:
            print("â­ï¸  Skipping generation (using existing pipeline)")
        
        # 2. Ğ—Ğ°Ğ¿ÑƒÑĞº ablation Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ
        try:
            results = run_ablation_for_dataset(dataset_name, args.n_runs, base_seed=args.seed, no_plots=args.no_plots, deterministic=args.deterministic, forced_task_type=args.task_type)
            all_dataset_results[dataset_name] = results
            
            # Ğ—Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ–Ğ² Ğ´Ğ»Ñ Ñ†ÑŒĞ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ñƒ
            dataset_output = output_dir / dataset_name
            dataset_output.mkdir(exist_ok=True, parents=True)
            
            # Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ– Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸
            detailed_results = []
            for config_name, config_results in results.items():
                for result in config_results:
                    result['dataset'] = dataset_name
                    result['configuration'] = config_name
                    detailed_results.append(result)
            
            df = pd.DataFrame(detailed_results)
            csv_path = dataset_output / f"results_{dataset_name}.csv"
            df.to_csv(csv_path, index=False)
            print(f"âœ… Results saved to {csv_path}")
            
        except Exception as e:
            print(f"âŒ Ablation failed: {e}")
            import traceback
            traceback.print_exc()
    
    # 3. Ğ—Ğ±Ñ–Ñ€ Ğ¿Ñ–Ğ´ÑÑƒĞ¼ĞºĞ¾Ğ²Ğ¸Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ–Ğ²
    print(f"\n{'='*80}")
    print("SUMMARY")
    print(f"{'='*80}")
    
    summary_data = []
    for dataset_name, results in all_dataset_results.items():
        for config_name, config_results in results.items():
            if config_results:
                accuracies = [r['accuracy'] for r in config_results]
                summary_data.append({
                    'dataset': dataset_name,
                    'configuration': config_name,
                    'mean_accuracy': np.mean(accuracies),
                    'std_accuracy': np.std(accuracies),
                    'n_runs': len(accuracies)
                })
    
    summary_df = pd.DataFrame(summary_data)
    summary_path = output_dir / "summary_all_datasets.csv"
    summary_df.to_csv(summary_path, index=False)
    
    if len(summary_df) > 0:
        print(f"\nğŸ“Š Summary statistics:")
        print(summary_df.pivot_table(
            values='mean_accuracy',
            index='configuration',
            columns='dataset',
            aggfunc='mean'
        ))
    else:
        print(f"\nâš ï¸  No results to summarize (all experiments failed)")
    
    print(f"\nâœ… All results saved to: {args.output_dir}")
    print(f"   Summary: {summary_path}")
    print("="*80)


if __name__ == "__main__":
    main()
