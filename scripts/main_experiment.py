#!/usr/bin/env python
"""
–ì–æ–ª–æ–≤–Ω–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–æ–≥–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—è –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤:
1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è ML pipeline —á–µ—Ä–µ–∑ Gemini API –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
2. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ–≥–æ –∫–æ–¥—É
3. –ü—Ä–æ–≤–µ–¥–µ–Ω–Ω—è ablation –∞–Ω–∞–ª—ñ–∑—É
4. –ó–±—ñ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è –≤—Å—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤

–ê–≤—Ç–æ—Ä: –§–µ—Ñ–µ–ª–æ–≤ –Ü–ª–ª—è –û–ª–µ–∫—Å–∞–Ω–¥—Ä–æ–≤–∏—á
–ú–ê–£–ü, 2025
"""

import os
import sys
import argparse
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List

import google.generativeai as genai
import numpy as np
import pandas as pd

# –î–æ–¥–∞—î–º–æ src –¥–æ —à–ª—è—Ö—É
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.mle_star_ablation.config import get_standard_configs
from src.mle_star_ablation.datasets import DatasetLoader
from src.mle_star_ablation.metrics import calculate_classification_metrics


def generate_pipeline_with_gemini(dataset_name: str, api_key: str) -> str:
    """
    –ì–µ–Ω–µ—Ä—É—î ML pipeline —á–µ—Ä–µ–∑ Gemini API –¥–ª—è –∑–∞–¥–∞–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É.
    
    Args:
        dataset_name: –ù–∞–∑–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç—É (breast_cancer, wine, digits, iris)
        api_key: Gemini API key
        
    Returns:
        str: –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π Python –∫–æ–¥ pipeline
    """
    # –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è Gemini
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.0-flash-exp")
    
    # –û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –¥–∞—Ç–∞—Å–µ—Ç
    dataset_info = DatasetLoader.get_dataset_info(dataset_name)
    
    # –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
    prompt = f"""
Generate a complete scikit-learn ML pipeline for the '{dataset_name}' dataset.

Dataset Information:
- Samples: {dataset_info['n_samples']}
- Features: {dataset_info['n_features']}
- Classes: {dataset_info['n_classes']}
- Task: Classification

Requirements:
1. Create a Pipeline with these steps:
   - 'preprocessor': Handle missing values and scaling (Pipeline with SimpleImputer + StandardScaler)
   - 'feature_engineering': Optional dimensionality reduction (e.g., PCA)
   - 'model': Classification model (LogisticRegression, RandomForest, SVC, etc.)

2. Return ONLY the Python function code that builds the pipeline like:

```python
def build_full_pipeline():
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer
    # ... other imports
    
    preprocessor = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    
    feature_engineering = Pipeline([
        # your feature engineering steps
    ])
    
    model = # your model choice
    
    return Pipeline([
        ('preprocessor', preprocessor),
        ('feature_engineering', feature_engineering),
        ('model', model)
    ])
```

3. Add detailed comments explaining:
   - Why you chose each component
   - What hyperparameters you selected and why
   - How each step helps for this specific dataset

4. Use random_state=42 where applicable
5. Choose appropriate model and hyperparameters for this dataset size and complexity
6. Keep preprocessing simple but effective

Generate ONLY the function code, no explanations outside the code.
"""
    
    print(f"\nüì° Generating pipeline for {dataset_name} via Gemini API...")
    response = model.generate_content(prompt)
    
    # –í–∏—Ç—è–≥—É—î–º–æ –∫–æ–¥ –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
    code = response.text
    
    # –Ø–∫—â–æ –∫–æ–¥ –æ–±–≥–æ—Ä–Ω—É—Ç–∏–π —É ```python, —Ä–æ–∑–ø–∞–∫–æ–≤—É—î–º–æ
    if "```python" in code:
        code = code.split("```python")[1].split("```")[0].strip()
    elif "```" in code:
        code = code.split("```")[1].split("```")[0].strip()
    
    return code


def save_generated_pipeline(code: str, dataset_name: str, output_dir: Path):
    """
    –ó–±–µ—Ä—ñ–≥–∞—î –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥ —É —Ñ–∞–π–ª.
    
    Args:
        code: –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π Python –∫–æ–¥
        dataset_name: –ù–∞–∑–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç—É
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    """
    output_dir.mkdir(exist_ok=True, parents=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = output_dir / f"pipeline_{dataset_name}_{timestamp}.py"
    
    # –§–æ—Ä–º—É—î–º–æ –ø–æ–≤–Ω–∏–π —Ñ–∞–π–ª –∑ header
    full_code = f'''"""
Generated ML Pipeline for {dataset_name} dataset
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Generator: Google Gemini 2.0 Flash Exp
"""

{code}


if __name__ == "__main__":
    # Test the pipeline
    from sklearn.datasets import load_{dataset_name}
    from sklearn.model_selection import cross_val_score
    
    X, y = load_{dataset_name}(return_X_y=True)
    pipeline = build_full_pipeline()
    
    scores = cross_val_score(pipeline, X, y, cv=3, scoring='accuracy')
    print(f"Pipeline for {dataset_name}:")
    print(f"  Accuracy: {{scores.mean():.3f}} ¬± {{scores.std():.3f}}")
'''
    
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(full_code)
    
    print(f"‚úÖ Saved to {filename}")
    return filename


def update_mle_star_pipeline(code: str, dataset_name: str):
    """
    –û–Ω–æ–≤–ª—é—î mle_star_generated_pipeline.py –Ω–æ–≤–∏–º –∫–æ–¥–æ–º.
    
    Args:
        code: –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥ pipeline
        dataset_name: –ù–∞–∑–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç—É (–¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—ó)
    """
    pipeline_file = Path(__file__).parent.parent / "src" / "mle_star_ablation" / "mle_star_generated_pipeline.py"
    
    # –ß–∏—Ç–∞—î–º–æ –ø–æ—Ç–æ—á–Ω–∏–π —Ñ–∞–π–ª
    with open(pipeline_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ —Å–µ–∫—Ü—ñ—é build_full_pipeline
    start_marker = "def build_full_pipeline("
    end_marker = "\n\n# ==== 2. –ù–ê–ó–í–ò –ö–†–û–ö–Ü–í"
    
    start_idx = content.find(start_marker)
    end_idx = content.find(end_marker)
    
    if start_idx == -1 or end_idx == -1:
        raise ValueError("Could not find build_full_pipeline section in the file")
    
    # –§–æ—Ä–º—É—î–º–æ –Ω–æ–≤–∏–π –∫–æ–¥ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—é —Å–∏–≥–Ω–∞—Ç—É—Ä–æ—é
    new_function = f'''def build_full_pipeline(
    numeric_features: Optional[List[str]] = None,
    categorical_features: Optional[List[str]] = None
) -> Pipeline:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î –ø–æ–≤–Ω–∏–π ML-–∫–æ–Ω–≤–µ—î—Ä, –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π Gemini API –¥–ª—è {dataset_name}.
    
    Pipeline —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑:
    1. Preprocessor: SimpleImputer + StandardScaler
    2. Feature Engineering: (–≤–∏–∑–Ω–∞—á–µ–Ω–æ Gemini)
    3. Model: (–≤–∏–∑–Ω–∞—á–µ–Ω–æ Gemini)
    
    –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ: Google Gemini 2.0 Flash Exp
    –î–∞—Ç–∞—Å–µ—Ç: {dataset_name}
    –î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    
    Returns:
        Pipeline: –ü–æ–≤–Ω–∏–π sklearn Pipeline –∑ —É—Å—ñ–º–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
    """
{code.replace("def build_full_pipeline():", "").strip()}
'''
    
    # –ó–∞–º—ñ–Ω—é—î–º–æ —Å—Ç–∞—Ä–∏–π –∫–æ–¥
    new_content = content[:start_idx] + new_function + content[end_idx:]
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ
    with open(pipeline_file, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    print(f"‚úÖ Updated {pipeline_file}")


def run_ablation_for_dataset(dataset_name: str, n_runs: int = 5) -> Dict:
    """
    –ó–∞–ø—É—Å–∫–∞—î ablation –∞–Ω–∞–ª—ñ–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É.
    
    Args:
        dataset_name: –ù–∞–∑–≤–∞ –¥–∞—Ç–∞—Å–µ—Ç—É
        n_runs: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä—ñ–≤ –∫–æ–∂–Ω–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó
        
    Returns:
        Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—Å—ñ—Ö –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π
    """
    from scripts.run_ablation import run_multiple_runs
    
    configs = get_standard_configs()
    results = {}
    
    print(f"\nüî¨ Running ablation analysis for {dataset_name}...")
    print(f"   Configurations: {len(configs)}")
    print(f"   Runs per config: {n_runs}")
    
    for idx, config in enumerate(configs, 1):
        config_name = config.get_name()
        print(f"   [{idx}/{len(configs)}] {config_name}...", end=" ")
        
        try:
            config_results = run_multiple_runs(
                config, dataset_name, None, None, n_runs
            )
            results[config_name] = config_results
            
            avg_acc = np.mean([r['accuracy'] for r in config_results])
            print(f"‚úì {avg_acc:.4f}")
            
        except Exception as e:
            print(f"‚úó Error: {e}")
            results[config_name] = []
    
    return results


def main():
    parser = argparse.ArgumentParser(
        description='–ê–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ç–∞ ablation –∞–Ω–∞–ª—ñ–∑ –¥–ª—è –≤—Å—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        '--datasets',
        type=str,
        nargs='+',
        default=['breast_cancer', 'wine', 'digits', 'iris'],
        help='–°–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É'
    )
    parser.add_argument(
        '--n-runs',
        type=int,
        default=5,
        help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä—ñ–≤ –∫–æ–∂–Ω–æ—ó –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='results_full_experiment',
        help='–î–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤'
    )
    parser.add_argument(
        '--skip-generation',
        action='store_true',
        help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—é pipeline (–≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —ñ—Å–Ω—É—é—á–∏–π)'
    )
    parser.add_argument(
        '--api-key',
        type=str,
        default=None,
        help='Gemini API key (–∞–±–æ —á–µ—Ä–µ–∑ GEMINI_API_KEY env)'
    )
    
    args = parser.parse_args()
    
    # API key
    api_key = args.api_key or os.getenv('GEMINI_API_KEY')
    if not api_key and not args.skip_generation:
        raise ValueError("Gemini API key required! Set --api-key or GEMINI_API_KEY env")
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)
    
    generated_dir = Path("generated_pipelines")
    
    print("="*80)
    print("AUTOMATED ABLATION EXPERIMENT")
    print("="*80)
    print(f"Datasets: {', '.join(args.datasets)}")
    print(f"Runs per config: {args.n_runs}")
    print(f"Output: {args.output_dir}")
    print("="*80)
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –¥–ª—è –≤—Å—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤
    all_dataset_results = {}
    
    for dataset_name in args.datasets:
        print(f"\n{'='*80}")
        print(f"DATASET: {dataset_name.upper()}")
        print(f"{'='*80}")
        
        # 1. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è pipeline (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)
        if not args.skip_generation:
            try:
                code = generate_pipeline_with_gemini(dataset_name, api_key)
                
                # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥
                save_generated_pipeline(code, dataset_name, generated_dir)
                
                # –û–Ω–æ–≤–ª—é—î–º–æ mle_star_generated_pipeline.py
                update_mle_star_pipeline(code, dataset_name)
                
                print("‚úÖ Pipeline generated and installed")
                
            except Exception as e:
                print(f"‚ùå Generation failed: {e}")
                continue
        else:
            print("‚è≠Ô∏è  Skipping generation (using existing pipeline)")
        
        # 2. –ó–∞–ø—É—Å–∫ ablation –∞–Ω–∞–ª—ñ–∑—É
        try:
            results = run_ablation_for_dataset(dataset_name, args.n_runs)
            all_dataset_results[dataset_name] = results
            
            # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è —Ü—å–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É
            dataset_output = output_dir / dataset_name
            dataset_output.mkdir(exist_ok=True, parents=True)
            
            # –î–µ—Ç–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            detailed_results = []
            for config_name, config_results in results.items():
                for result in config_results:
                    result['dataset'] = dataset_name
                    result['configuration'] = config_name
                    detailed_results.append(result)
            
            df = pd.DataFrame(detailed_results)
            csv_path = dataset_output / f"results_{dataset_name}.csv"
            df.to_csv(csv_path, index=False)
            print(f"‚úÖ Results saved to {csv_path}")
            
        except Exception as e:
            print(f"‚ùå Ablation failed: {e}")
            import traceback
            traceback.print_exc()
    
    # 3. –ó–±—ñ—Ä –ø—ñ–¥—Å—É–º–∫–æ–≤–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
    print(f"\n{'='*80}")
    print("SUMMARY")
    print(f"{'='*80}")
    
    summary_data = []
    for dataset_name, results in all_dataset_results.items():
        for config_name, config_results in results.items():
            if config_results:
                accuracies = [r['accuracy'] for r in config_results]
                summary_data.append({
                    'dataset': dataset_name,
                    'configuration': config_name,
                    'mean_accuracy': np.mean(accuracies),
                    'std_accuracy': np.std(accuracies),
                    'n_runs': len(accuracies)
                })
    
    summary_df = pd.DataFrame(summary_data)
    summary_path = output_dir / "summary_all_datasets.csv"
    summary_df.to_csv(summary_path, index=False)
    
    print(f"\nüìä Summary statistics:")
    print(summary_df.pivot_table(
        values='mean_accuracy',
        index='configuration',
        columns='dataset',
        aggfunc='mean'
    ))
    
    print(f"\n‚úÖ All results saved to: {args.output_dir}")
    print(f"   Summary: {summary_path}")
    print("="*80)


if __name__ == "__main__":
    main()
