"""
Generated by: gemini-2.5-pro
Timestamp: 2025-11-13T12:41:45.142519
Generation time: 39.30s
"""

def build_full_pipeline():
    """
    Builds a complete scikit-learn machine learning pipeline for the breast cancer dataset.

    The pipeline consists of three main stages:
    1. Preprocessing: Handles missing values and scales the data.
    2. Feature Engineering: Reduces dimensionality using Principal Component Analysis (PCA).
    3. Model: A Support Vector Classifier (SVC) for binary classification.

    Returns:
        sklearn.pipeline.Pipeline: The fully constructed scikit-learn pipeline.
    """
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer
    from sklearn.decomposition import PCA
    from sklearn.svm import SVC

    # --- Step 1: Preprocessor ---
    # This sub-pipeline handles the initial data cleaning and preparation.
    # The breast_cancer dataset from sklearn doesn't have missing values, but including
    # an imputer is a best practice for creating robust pipelines that can handle
    # real-world, imperfect data.
    preprocessor = Pipeline(steps=[
        # 'imputer': Fills in any missing values (NaNs) in the dataset.
        # Strategy 'mean' is chosen as it's a simple and effective default for
        # numerical data. It replaces missing values with the mean of their respective column.
        ('imputer', SimpleImputer(strategy='mean')),

        # 'scaler': Scales the features to have zero mean and unit variance.
        # This is crucial for many ML algorithms, especially distance-based ones like SVC
        # and regularized models. It ensures that features with larger scales (e.g., 'mean area')
        # do not dominate features with smaller scales (e.g., 'mean smoothness').
        ('scaler', StandardScaler())
    ])

    # --- Step 2: Feature Engineering ---
    # This sub-pipeline is for transforming features, in this case, dimensionality reduction.
    feature_engineering = Pipeline(steps=[
        # 'pca': Principal Component Analysis for dimensionality reduction.
        # The 30 features in the breast cancer dataset are measurements of cell nuclei
        # and many are highly correlated (e.g., radius, perimeter, and area).
        # PCA creates a new set of uncorrelated features (principal components)
        # that capture the most variance in the data.
        #
        # Hyperparameter `n_components=0.95`:
        # Instead of picking a fixed number of components, we instruct PCA to
        # automatically select the minimum number of components required to
        # explain at least 95% of the variance in the data. This is a data-driven
        # way to reduce noise and redundancy without losing significant information.
        #
        # `random_state=42` is used for reproducibility, as PCA can use a randomized
        # solver depending on the data shape and `n_components`.
        ('pca', PCA(n_components=0.95, random_state=42))
    ])

    # --- Step 3: Model ---
    # The final step is the classification model.
    # A Support Vector Classifier (SVC) is chosen. SVC is a powerful model that
    # works well on smaller, high-dimensional datasets like this one. It excels at
    # finding a clear margin of separation between classes.
    model = SVC(
        # `kernel='rbf'`: The Radial Basis Function kernel is a non-linear kernel
        # that allows the SVC to find complex, non-linear decision boundaries,
        # which is often necessary for real-world data.
        kernel='rbf',

        # `C=1.0`: The regularization parameter. It controls the trade-off between
        # achieving a low training error and a low testing error (generalization).
        # A value of 1.0 is a good, standard starting point.
        C=1.0,

        # `gamma='scale'`: The kernel coefficient. The 'scale' option is a good
        # default that sets gamma to a value that scales with the number of features
        # and the variance of the data, making it less sensitive to the feature scale.
        gamma='scale',

        # `probability=True`: This enables the model to predict class probabilities
        # using `predict_proba()`. It's useful for evaluating the model with metrics
        # like ROC AUC or for understanding the model's confidence.
        probability=True,

        # `random_state=42`: Ensures reproducibility. While the core SVM algorithm is
        # deterministic, this affects the probability estimation process.
        random_state=42
    )

    # --- Assemble the Full Pipeline ---
    # The final pipeline chains all the steps together. When we call .fit() on this
    # pipeline, data flows through each step sequentially:
    # 1. It's imputed and scaled by the 'preprocessor'.
    # 2. Its dimensionality is reduced by 'feature_engineering'.
    # 3. The transformed data is used to train the 'model'.
    full_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('feature_engineering', feature_engineering),
        ('model', model)
    ])

    return full_pipeline