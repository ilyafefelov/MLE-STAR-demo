"""
Generated by: gemini-2.5-flash
Timestamp: 2025-11-13T12:32:25.473740
Generation time: 16.18s
"""

def build_full_pipeline():
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer
    from sklearn.decomposition import PCA
    from sklearn.ensemble import RandomForestClassifier

    # Preprocessing Pipeline: Handles missing values and scales features.
    preprocessor = Pipeline([
        # SimpleImputer: This step handles potential missing values in the dataset.
        # Although the 'breast_cancer' dataset is typically complete, including an imputer
        # makes the pipeline robust for similar datasets or future data that might have gaps.
        # 'strategy='mean'' is a suitable choice for numerical features, replacing NaNs
        # with the mean of their respective column.
        ('imputer', SimpleImputer(strategy='mean')),

        # StandardScaler: This scales the features to have zero mean and unit variance.
        # Scaling is critical for many machine learning algorithms (like PCA, SVMs,
        # and Logistic Regression) as they are sensitive to the magnitude of features.
        # It helps prevent features with larger numerical ranges from dominating
        # the learning process and often improves model convergence and performance.
        ('scaler', StandardScaler())
    ])

    # Feature Engineering Pipeline: Optional dimensionality reduction.
    feature_engineering = Pipeline([
        # PCA (Principal Component Analysis): This transformer reduces the dimensionality
        # of the dataset by transforming the features into a new set of orthogonal
        # components, which capture the most variance in the data.
        # For the 'breast_cancer' dataset with 30 features, PCA can be beneficial by:
        # 1. Reducing noise and multicollinearity among highly correlated features.
        # 2. Potentially improving model training speed and generalization by focusing
        #    on the most informative components.
        # 'n_components=0.95' instructs PCA to select the minimum number of principal
        # components such that 95% of the total variance in the original data is retained.
        # This is a common heuristic to balance dimensionality reduction with information preservation.
        # 'random_state=42' ensures reproducibility of the PCA transformation.
        ('pca', PCA(n_components=0.95, random_state=42))
    ])

    # Classification Model: RandomForestClassifier chosen for its robustness.
    # RandomForestClassifier: An ensemble learning method that constructs a multitude
    # of decision trees at training time and outputs the class that is the mode
    # of the classes (classification) or mean prediction (regression) of the individual trees.
    # It's a powerful and robust model, well-suited for binary classification tasks like
    # breast cancer prediction. It handles non-linear relationships effectively and
    # generally performs well out-of-the-box on datasets of this size (569 samples, 30 features).
    #
    # Hyperparameters:
    # - n_estimators=100: The number of trees in the forest. 100 is a good default
    #   that often provides a stable prediction without excessively increasing computation time.
    #   More trees generally reduce variance but offer diminishing returns.
    # - max_depth=10: The maximum depth of the individual decision trees. Limiting
    #   the depth helps to prevent individual trees from overfitting to the training data.
    #   For this dataset size and complexity, a moderate depth like 10 is a reasonable
    #   starting point to balance bias and variance.
    # - random_state=42: Ensures reproducibility of the random forest's internal
    #   randomness (e.g., feature sampling, bootstrap sampling for each tree).
    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

    # Combine all steps into a final comprehensive pipeline.
    # The data will sequentially pass through preprocessing, then feature engineering,
    # and finally be used to train/predict with the chosen classification model.
    return Pipeline([
        ('preprocessor', preprocessor),
        ('feature_engineering', feature_engineering),
        ('model', model)
    ])