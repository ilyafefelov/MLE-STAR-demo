"""
Generated by: gemini-2.5-flash-lite
Timestamp: 2025-11-13T12:40:28.419498
Generation time: 3.73s
"""

def build_full_pipeline():
    """
    Builds a complete scikit-learn ML pipeline for the breast_cancer dataset.

    The pipeline includes preprocessing (imputation and scaling), optional
    dimensionality reduction (PCA), and a classification model.

    Returns:
        sklearn.pipeline.Pipeline: The configured scikit-learn pipeline.
    """
    from sklearn.pipeline import Pipeline
    from sklearn.preprocessing import StandardScaler
    from sklearn.impute import SimpleImputer
    from sklearn.decomposition import PCA
    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier

    # --- Preprocessing Pipeline ---
    # Rationale: The breast_cancer dataset is generally clean with no missing values.
    # However, including imputation makes the pipeline more robust for other datasets
    # and is good practice. StandardScaler is crucial for distance-based algorithms
    # (like SVC, or even Logistic Regression if regularization is used) and can also
    # help gradient-based methods converge faster.

    # SimpleImputer: Handles potential missing values by replacing them with the mean.
    # Strategy='mean' is a common and reasonable choice for numerical data.
    # StandardScaler: Scales features to have zero mean and unit variance. This is
    # important for models sensitive to feature scales.
    preprocessor = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])

    # --- Feature Engineering Pipeline ---
    # Rationale: The breast_cancer dataset has 30 features. While not extremely high-dimensional,
    # PCA can be used to reduce dimensionality, potentially improving model performance by
    # removing noise, reducing overfitting, and speeding up training. For this dataset size,
    # reducing to a smaller number of components is often beneficial.

    # PCA: Principal Component Analysis for dimensionality reduction.
    # n_components=0.95: This is a common heuristic to retain 95% of the variance.
    # It allows PCA to automatically select the number of components needed to explain
    # that much variance. This is a good balance between reduction and information loss.
    # random_state=42: Ensures reproducibility of the PCA transformation.
    feature_engineering = Pipeline([
        ('pca', PCA(n_components=0.95, random_state=42))
    ])

    # --- Model Selection ---
    # Rationale: Logistic Regression and RandomForestClassifier are both strong
    # candidates for binary classification tasks like this.
    # Logistic Regression is a good baseline, interpretable, and efficient.
    # RandomForestClassifier is more powerful, can capture non-linear relationships,
    # and is generally less prone to overfitting than single decision trees.
    # Given the dataset size and complexity, both are suitable. We'll choose Logistic Regression
    # for this example as a solid and computationally efficient choice, with parameters
    # set to avoid overfitting.

    # LogisticRegression: A linear model for binary classification.
    # solver='liblinear': A good default solver for small datasets.
    # C=1.0: The inverse of regularization strength. A default value, assuming
    #   moderate regularization. For this dataset size, it's a reasonable starting point.
    # random_state=42: Ensures reproducibility of the model training.
    model = LogisticRegression(solver='liblinear', C=1.0, random_state=42)

    # Alternative Model: RandomForestClassifier
    # model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5, min_samples_split=2)
    #   n_estimators=100: A common choice for the number of trees.
    #   max_depth=5: Limiting tree depth helps prevent overfitting on this dataset.
    #   min_samples_split=2: Default value, reasonable for this dataset size.


    # --- Full Pipeline Construction ---
    # The pipeline chains the preprocessing, feature engineering, and model steps.
    # Each step is named for clarity and easy access.
    full_pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('feature_engineering', feature_engineering),
        ('model', model)
    ])

    return full_pipeline