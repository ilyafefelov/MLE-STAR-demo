# Global Experiment Report: Empirical Verification of LLM-Generated Pipeline Complexity

**Date:** November 20, 2025
**Agent Model:** Gemini 2.5 Flash Lite
**Protocol:** N=20 runs per experiment, 95% Confidence Intervals

## 1. Executive Summary

This report presents the final results of an empirical study designed to verify the **Over-engineering Hypothesis** in LLM-generated Machine Learning pipelines. The study analyzed the performance of the ADK Agent across 10 diverse datasets (5 classification, 5 regression) using an ablation study methodology.

**Key Conclusion:** The hypothesis is **confirmed**. In the majority of cases, simplified pipeline configurations (`minimal`, `no_feature_engineering`) outperformed or matched the complex `full` configurations generated by the LLM. The agent demonstrated a systematic tendency to introduce unnecessary complexity (ensemble methods, elaborate feature engineering) that often degraded performance.

## 2. Methodology

*   **Agent:** Google Gemini 2.5 Flash Lite (chosen for speed/quality balance).
*   **Evaluation Protocol:** Each dataset was tested with a "Full" pipeline (Agent's best attempt) and several ablated variants (`no_feature_engineering`, `no_scaling`, `no_ensemble`, `minimal`).
*   **Statistical Rigor:** Experiments were repeated 20 times ($N=20$) to calculate Mean scores, Standard Deviations, and Confidence Intervals.
*   **Metrics:** Accuracy for Classification; $R^2$ score for Regression.

## 3. Key Findings: The Cost of Complexity

### 3.1. Confirmation of Over-engineering
The most consistent finding is that "Full" pipelines rarely achieve the best results.

*   **Synthetic Nonlinear Data:** The `minimal` configuration outperformed the `full` pipeline by a massive margin (**+0.4076** $R^2$). The agent's attempt to model non-linearity via complex feature engineering failed compared to a simple, robust model.
*   **Iris Dataset:** The `full` pipeline (including PCA and GridSearch) performed **5.5% worse** than the `no_feature_engineering` baseline (0.9083 vs 0.9633).
*   **California Housing (Ensemble Sabotage):** The `full` configuration achieved an $R^2$ of **0.637**, while the `minimal` configuration reached **0.844**. Analysis reveals this was due to a poor ensemble strategy where a weak Linear Regression model dragged down the performance of a strong LightGBM model.

### 3.2. The Critical Role of Scaling
Ablation studies highlighted that while "advanced" features are often redundant, fundamental preprocessing like Scaling is non-negotiable for certain datasets.

*   **Breast Cancer:** Removing scaling caused accuracy to drop by **10.4%** (0.956 $\to$ 0.852).
*   **Wine:** Accuracy plummeted from **0.983** to **0.721** without scaling.
*   **California Housing:** The model completely failed without scaling ($R^2 \approx -0.001$).

### 3.3. The Diabetes Counter-Example
The **Diabetes** dataset was an exception where the `full` pipeline marginally outperformed simplified versions (0.475 vs 0.467), though the difference falls within the margin of error (Std Dev $\approx$ 0.06). This suggests that for noisy, real-world data, the agent's complexity does not necessarily hurt, but it also fails to provide statistically significant gains over simpler approaches.

## 4. Agent vs. One-Shot Generation (Iris Case Study)

A direct comparison was conducted between the ADK Agent (iterative) and Gemini One-Shot (zero-shot) generation for the Iris dataset.

*   **Gemini One-Shot (Best):** ~0.989 Accuracy
*   **ADK Agent (Best):** ~0.978 Accuracy

The One-Shot approach slightly outperformed the Agent, further supporting the idea that the Agent's complex iterative process may introduce "noise" into the solution for simple problems.

## 5. Detailed Results Summary

### Classification Tasks

| Dataset | Best Config | Best Score | Full Pipeline Score | Impact of Complexity |
|:---|:---|---:|---:|:---|
| **Breast Cancer** | `no_feature_engineering` | 0.9557 | 0.9487 | **Negative** (-0.0070) |
| **Wine** | `no_tuning` | 0.9833 | 0.9722 | **Negative** (-0.0111) |
| **Digits** | `no_feature_engineering` | 0.9822 | 0.9790 | **Negative** (-0.0032) |
| **Iris** | `no_feature_engineering` | 0.9633 | 0.9083 | **Negative** (-0.0550) |
| **Synthetic** | `no_feature_engineering` | 0.8867 | 0.8835 | **Neutral** (-0.0032) |

### Regression Tasks

| Dataset | Best Config | Best Score ($R^2$) | Full Pipeline Score | Impact of Complexity |
|:---|:---|---:|---:|:---|
| **California Housing** | `minimal` | 0.8440 | 0.6366 | **Catastrophic** (-0.2074) |
| **Diabetes** | `full` | 0.4753 | 0.4753 | **Neutral/Positive** |
| **Synth. Easy** | `no_feature_engineering` | 0.9804 | 0.9684 | **Negative** (-0.0120) |
| **Synth. Medium** | `no_feature_engineering` | 0.8033 | 0.7632 | **Negative** (-0.0401) |
| **Synth. Nonlinear** | `minimal` | 0.8507 | 0.4431 | **Catastrophic** (-0.4076) |

## 6. Discussion & Analogy

The findings strongly support the **Bias-Variance Tradeoff** theory. The LLM-Agent acts like an **over-enthusiastic architect** who adds unnecessary load-bearing walls and decorative columns to a small house.
*   **Ablation as Stress Testing:** When we remove these "extra" components (ablation), the structure often becomes more stable and efficient.
*   **Structural Integrity:** In cases like *California Housing*, the "decorations" (bad ensembles) actually compromised the structural integrity of the building, causing it to collapse (lower performance).

## 7. Conclusion

The ADK Agent demonstrates a clear bias towards **complexity**. While capable of generating functional pipelines, it systematically over-engineers solutions. Future development should focus on:
1.  **Parsimony Bias:** Instructing the agent to prefer simpler models unless complexity is proven necessary.
2.  **Ensemble Validation:** Implementing stricter checks before adding models to an ensemble.
3.  **Adaptive Scaling:** Ensuring scaling is always applied when sensitive models (SVM, KNN, Linear) are used.

---
*See `reports/aggregate_summary_stats.csv` for raw data.*
